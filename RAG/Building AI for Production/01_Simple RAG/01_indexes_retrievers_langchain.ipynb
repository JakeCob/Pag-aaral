{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3027dc8",
   "metadata": {},
   "source": [
    "# LangChain’s indexes and retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00511b9",
   "metadata": {},
   "source": [
    "LangChain’s indexes and retrievers provide modular, adaptable, and customizable options for handling unstructured data with LLMs. The primary index types in LangChain are based on vector databases, mainly emphasizing indexes using embeddings.\n",
    "The role of retrievers is to extract relevant documents for integration into language model prompts. In LangChain, a retriever employs a get_relevant_documents method, taking a query string as input and generating a list of documents that are relevant to that query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d94a7",
   "metadata": {},
   "source": [
    "### Install the necessary Python packages and use the TextLoader class to load text files and create a LangChain Document object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ba87bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==1.2.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain-community==0.4.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-text-splitters==1.1.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: openai in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (0.27.8)\n",
      "Requirement already satisfied: tiktoken in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dotenv in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: deeplake<4.0.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake[enterprise]<4.0.0) (3.9.52)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain==1.2.0) (1.2.6)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain==1.2.0) (1.0.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain==1.2.0) (2.12.5)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-community==0.4.1) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-community==0.4.1) (2.0.45)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-community==0.4.1) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-community==0.4.1) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-community==0.4.1) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-community==0.4.1) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-community==0.4.1) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-community==0.4.1) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-community==0.4.1) (0.6.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-community==0.4.1) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-community==0.4.1) (1.26.4)\n",
      "Requirement already satisfied: pillow~=10.4.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (10.4.0)\n",
      "Requirement already satisfied: boto3 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (1.40.61)\n",
      "Requirement already satisfied: click in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (8.3.1)\n",
      "Requirement already satisfied: six~=1.16.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (1.16.0)\n",
      "Requirement already satisfied: pathos in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (0.3.4)\n",
      "Requirement already satisfied: humbug>=0.3.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (0.3.2)\n",
      "Requirement already satisfied: tqdm in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (4.67.1)\n",
      "Requirement already satisfied: lz4 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (4.4.5)\n",
      "Requirement already satisfied: pyjwt in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (2.10.1)\n",
      "Requirement already satisfied: aioboto3>=10.4.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (15.5.0)\n",
      "Requirement already satisfied: nest_asyncio in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (1.6.0)\n",
      "Requirement already satisfied: libdeeplake in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from deeplake<4.0.0->deeplake[enterprise]<4.0.0) (0.0.164)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain==1.2.0) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain==1.2.0) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain==1.2.0) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain==1.2.0) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain==1.2.0) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (0.3.1)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community==0.4.1) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community==0.4.1) (0.25.0)\n",
      "Requirement already satisfied: anyio in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (4.12.0)\n",
      "Requirement already satisfied: certifi in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.2.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.2.0) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.2.0) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.5->langchain-community==0.4.1) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.5->langchain-community==0.4.1) (2.6.2)\n",
      "Requirement already satisfied: greenlet>=1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community==0.4.1) (3.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1) (1.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: aiobotocore==2.25.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiobotocore[boto3]==2.25.1->aioboto3>=10.4.0->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (2.25.1)\n",
      "Requirement already satisfied: aiofiles>=23.2.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aioboto3>=10.4.0->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (25.1.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiobotocore==2.25.1->aiobotocore[boto3]==2.25.1->aioboto3>=10.4.0->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (0.13.0)\n",
      "Requirement already satisfied: botocore<1.40.62,>=1.40.46 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiobotocore==2.25.1->aiobotocore[boto3]==2.25.1->aioboto3>=10.4.0->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (1.40.61)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiobotocore==2.25.1->aiobotocore[boto3]==2.25.1->aioboto3>=10.4.0->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiobotocore==2.25.1->aiobotocore[boto3]==2.25.1->aioboto3>=10.4.0->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from aiobotocore==2.25.1->aiobotocore[boto3]==2.25.1->aioboto3>=10.4.0->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (1.17.3)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from boto3->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (0.14.0)\n",
      "Requirement already satisfied: dill in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from libdeeplake->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (0.4.0)\n",
      "Requirement already satisfied: ppft>=1.7.7 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from pathos->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (1.7.7)\n",
      "Requirement already satisfied: pox>=0.3.6 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from pathos->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.18 in /root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages (from pathos->deeplake<4.0.0->deeplake[enterprise]<4.0.0) (0.70.18)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain==1.2.0 langchain-community==0.4.1 langchain-text-splitters==1.1.0 openai tiktoken python-dotenv \"deeplake[enterprise]<4.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a36c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764980a",
   "metadata": {},
   "source": [
    "### text to write to a local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a734627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Google opens up its AI language model PaLM to challenge OpenAI and GPT-3 Google offers developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses \"generate text, images, code, videos, audio, and more from simple natural language prompts.\"PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or Meta\\'s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs, PaLM is a flexible system that can potentially carry out all sorts of text generation and editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for example, or you could use it for tasks like summarizing text or even writing code. (It\\'s similar to features Google also announced today for its Workspace apps like Google Docs and Gmail.)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =\"\"\" Google opens up its AI language model PaLM to challenge OpenAI and GPT-3 Google offers developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses \"generate text, images, code, videos, audio, and more from simple natural language prompts.\"PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or Meta's LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs, PaLM is a flexible system that can potentially carry out all sorts of text generation and editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for example, or you could use it for tasks like summarizing text or even writing code. (It's similar to features Google also announced today for its Workspace apps like Google Docs and Gmail.)\"\"\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de84bdb",
   "metadata": {},
   "source": [
    "### write text to local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d13ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"my_file.txt\", \"w\") as file: file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb880743",
   "metadata": {},
   "source": [
    "# use TextLoader to load text from local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1471bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"my_file.txt\")\n",
    "docs_from_file = loader.load()\n",
    "\n",
    "print(len(docs_from_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487efd2",
   "metadata": {},
   "source": [
    "### Use CharacterTextSplitter to split the documents into text snippets called “chunks.” Chunk_overlap is the number of characters that overlap between two chunks. It preserves context and improves coherence by ensuring that important information is not cut off at the boundaries of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f04f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e406b",
   "metadata": {},
   "source": [
    "### create a text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758f81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49265873",
   "metadata": {},
   "source": [
    "### split documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7d4c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'my_file.txt'}, page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3 Google offers developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses \"generate text, images, code, videos, audio, and more from simple natural language prompts.\"PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or Meta\\'s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs, PaLM is a flexible system that can potentially carry out all sorts of text generation and editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for example, or you could use it for tasks like summarizing text or even writing code. (It\\'s similar to features Google also announced today for its Workspace apps like Google Docs and Gmail.)')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(docs_from_file)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024484df",
   "metadata": {},
   "source": [
    "### Create a vector embedding for each text snippet.\n",
    "\n",
    "These embeddings allow us to effectively search for documents or portions of documents that relate to our query by examining their semantic similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97d6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0192a12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1390309/3280910429.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-openai package and should be used instead. To use it run `pip install -U `langchain-openai` and import as `from `langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb63338",
   "metadata": {},
   "source": [
    "### Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fcf7f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages/humbug/report.py:47: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources  # type: ignore\n",
      "/root/anaconda3/envs/pagaaral-311/lib/python3.11/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (4.4.4) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DeepLake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b854c",
   "metadata": {},
   "source": [
    "### Create Deep Lake Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a66a19e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1390309/841058227.py:6: LangChainDeprecationWarning: This class is deprecated and will be removed in a future version. You can swap to using the `DeeplakeVectorStore` implementation in `langchain-deeplake`. Please do not submit further PRs to this class.See <https://github.com/activeloopai/langchain-deeplake>\n",
      "  db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://rafaljacobmatthew/langchain_course_indexers_retrievers already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 1 embeddings in 1 batches of size 1:: 100%|██████████| 1/1 [00:42<00:00, 42.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://rafaljacobmatthew/langchain_course_indexers_retrievers', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (1, 1536)  float32   None   \n",
      "    id        text      (1, 1)      str     None   \n",
      " metadata     json      (1, 1)      str     None   \n",
      "   text       text      (1, 1)      str     None   \n",
      "<langchain_community.vectorstores.deeplake.DeepLake object at 0x7f59268f1b90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "my_activeloop_org_id = os.getenv(\"ACTIVELOOP_ORG_ID\")\n",
    "my_activeloop_dataset_name = \"langchain_course_indexers_retrievers\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# Add documents to the Deep Lake dataset\n",
    "db.add_documents(docs)\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9ba67",
   "metadata": {},
   "source": [
    "### Create retriever from db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9606868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['DeepLake', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.deeplake.DeepLake object at 0x7f59268f1b90>, search_kwargs={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94107b6",
   "metadata": {},
   "source": [
    "### Use the RetrievalQA class to define a question answering chain using external data source and start with question-answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3921da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43067d",
   "metadata": {},
   "source": [
    "### Create a retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb4f2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7035286c",
   "metadata": {},
   "source": [
    "### Query our document about a specific topic found in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea92a422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1390309/2296197511.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use `invoke` instead.\n",
      "  response = qa_chain.run(query)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Google plans to challenge OpenAI by offering developers access to its advanced AI language model PaLM, which is similar to OpenAI's GPT series. By launching an API for PaLM and providing AI enterprise tools for businesses to generate text, images, code, videos, audio, and more from natural language prompts, Google aims to compete with OpenAI in the field of AI language models.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How Google plans to challenge OpenAI?\"\n",
    "response = qa_chain.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2384a5",
   "metadata": {},
   "source": [
    "## Behind The Scenes\n",
    "\n",
    "In creating the retriever stages, we set the `chain_type` to \"stuff.\" This is the most straightforward document chain (\"stuff\" as in \"to stuff\" or \"to fill\"). It takes a list of documents, inserts them all into a prompt, and passes that prompt to an LLM. This approach is only efficient with shorter documents due to the context length limitations of most LLMs.\n",
    "\n",
    "The process also involves conducting a similarity search using embeddings to find documents that match and can be used as context for the LLM. While this might appear limited in scope with a single document, its effectiveness is enhanced when dealing with multiple documents segmented into \"chunks.\" We can supply the LLM with the relevant information within its context size by selecting the most relevant documents based on semantic similarity.\n",
    "\n",
    "This example highlighted the critical role of indexes and retrievers in augmenting the performance of LLMs when managing document-based data. The system's efficiency in sourcing and presenting relevant information is increased by transforming documents and user queries into numerical vectors (embeddings) and storing these in specialized databases like Deep Lake.\n",
    "\n",
    "The effectiveness of this approach in enhancing the language comprehension of Large Language Models (LLMs) is underscored by the retriever's ability to pinpoint documents closely related to a user's query in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bvhth8dwyav",
   "metadata": {},
   "source": [
    "## A Potential Problem\n",
    "\n",
    "This method poses a notable challenge, especially when dealing with a more extensive data set. In the example, the text was divided into equal parts, which resulted in both relevant and irrelevant text being presented in response to a user's query.\n",
    "\n",
    "Incorporating unrelated content in the LLM prompt can be problematic for two main reasons:\n",
    "\n",
    "1. It may distract the LLM from focusing on essential details.\n",
    "2. It consumes space in the prompt that could be allocated to more relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nsxggyyqdc",
   "metadata": {},
   "source": [
    "## Possible Solution: Contextual Compression\n",
    "\n",
    "A `DocumentCompressor` can address this issue. Instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query so that only the relevant information is returned. \"Compressing\" here refers to compressing an individual document's contents and filtering out documents wholesale.\n",
    "\n",
    "The `ContextualCompressionRetriever` serves as a wrapper for another retriever within LangChain. It combines a base retriever with a `DocumentCompressor`, ensuring that only the most pertinent segments of the documents retrieved by the base retriever are presented in response to a specific query.\n",
    "\n",
    "A standard tool that can use the compressor is `LLMChainExtractor`. This tool employs an LLMChain to isolate only those statements from the documents that are relevant to the query. A `ContextualCompressionRetriever`, incorporating an `LLMChainExtractor`, is utilized to enhance the document retrieval process. The `LLMChainExtractor` reviews the initially retrieved documents and selectively extracts content directly relevant to the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0rpq34mqo1an",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# create GPT wrapper\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# create compressor for the retriever\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eq7m4asp73q",
   "metadata": {},
   "source": [
    "### Retrieve Compressed Documents\n",
    "\n",
    "Once the `compression_retriever` is created, we can retrieve the relevant compressed documents for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "okpc9vu1kpr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google opens up its AI language model PaLM to challenge OpenAI and GPT-3. Google offers developers access to one of its most advanced AI language models: PaLM. PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI. Google first announced PaLM in April 2022.\n"
     ]
    }
   ],
   "source": [
    "# retrieving compressed documents\n",
    "retrieved_docs = compression_retriever.invoke(\n",
    "    \"How Google plans to challenge OpenAI?\"\n",
    ")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6nj92cgpei",
   "metadata": {},
   "source": [
    "Compressors try to simplify the process by sending only essential data to the LLM. This also allows you to provide more information to the LLM. Letting the compressors handle precision during the initial retrieval step will allow you to focus on recall (for example, by increasing the number of documents returned).\n",
    "\n",
    "We saw how it is possible to create a retriever from a `.txt` file; however, data can come in different types. The LangChain framework offers diverse classes that enable data to be loaded from multiple sources, including PDFs, URLs, and Google Drive, among others, which we will explore later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0c49f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pagaaral-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
