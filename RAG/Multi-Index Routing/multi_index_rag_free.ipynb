{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Index Routing RAG with PDF Documents - FREE Open Source Version\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "In this notebook, you will learn:\n",
    "\n",
    "1. **Multi-Index RAG Architecture** - How to route queries to domain-specific indexes\n",
    "2. **PDF Processing** - Load and process real PDF documents\n",
    "3. **Document Chunking Strategies** - Split documents effectively for retrieval\n",
    "4. **LLM-Based Routing** - Use AI to intelligently route queries\n",
    "5. **Citation Tracking** - Track sources and page numbers for answers\n",
    "6. **Evaluation Metrics** - Measure system performance\n",
    "\n",
    "## 📚 Dataset\n",
    "\n",
    "We'll process 4 PDF documents from `pdf_documents/` folder:\n",
    "- **Legal/Compliance Domain**: 3 PDPA Advisory Guidelines\n",
    "- **HR/Business Domain**: 1 Employee Handbook\n",
    "- **General Domain**: Fallback for other queries\n",
    "\n",
    "## 🔧 Prerequisites\n",
    "\n",
    "**100% FREE Tools Used:**\n",
    "- **Ollama** - Local LLM (no API key needed!)\n",
    "- **Sentence Transformers** - Free embedding models\n",
    "- **ChromaDB** - Open-source vector database\n",
    "\n",
    "### Installation:\n",
    "1. Install Ollama: `curl -fsSL https://ollama.com/install.sh | sh`\n",
    "2. Pull a model: `ollama pull llama3.2` or `ollama pull mistral`\n",
    "3. Install Python packages (next cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Setup and Dependencies\n",
    "\n",
    "First, let's install and import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Installing packages (this may take 1-2 minutes)...\n",
      "\n",
      "Requirement already satisfied: langchain in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-ollama in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (0.3.10)\n",
      "Requirement already satisfied: langchain-core in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (0.3.79)\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
      "Requirement already satisfied: matplotlib in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (3.10.7)\n",
      "Requirement already satisfied: plotly in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (6.3.1)\n",
      "Requirement already satisfied: pandas in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: chromadb in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: pypdf in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (6.1.1)\n",
      "Requirement already satisfied: sentence-transformers in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (5.1.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain) (0.4.34)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain) (2.12.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain) (2.0.44)\n",
      "Requirement already satisfied: requests<3,>=2 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: greenlet>=1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-community) (3.13.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-community) (2.3.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-ollama) (0.6.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-huggingface) (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from langchain-huggingface) (0.35.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from plotly) (2.7.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (1.23.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (1.75.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (0.19.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from sentence-transformers) (4.57.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: filelock in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.10)\n",
      "Requirement already satisfied: pyproject_hooks in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.41.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.1)\n",
      "Requirement already satisfied: sympy in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: networkx in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "✅ Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install packages in the current kernel's Python environment\n",
    "# Removed -q flag to show installation progress\n",
    "import sys\n",
    "\n",
    "print(\"📦 Installing packages (this may take 1-2 minutes)...\\n\")\n",
    "\n",
    "!{sys.executable} -m pip install langchain langchain-community langchain-ollama langchain-core langchain-huggingface matplotlib plotly pandas chromadb pypdf sentence-transformers\n",
    "\n",
    "print(\"\\n✅ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/multi_index_rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All dependencies loaded successfully!\n",
      "📁 Working directory: /root/Programming Projects/Personal/Pag-aaral/RAG/Multi-Index Routing\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Literal, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # ✅ Updated import\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "print(\"✅ All dependencies loaded successfully!\")\n",
    "print(f\"📁 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Ollama Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️  Local Ollama not installed (this is fine if using Railway)\n",
      "\n",
      "💡 You're using Railway Ollama, so local installation is optional.\n",
      "\n",
      "📝 If you want to install Ollama locally for development:\n",
      "   curl -fsSL https://ollama.com/install.sh | sh\n",
      "   ollama pull llama3.2:3b\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is installed LOCALLY (Optional - skip if using Railway)\n",
    "# Since you're using Railway Ollama, this will show \"command not found\" - that's OK!\n",
    "\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "    print(\"✅ Local Ollama detected:\")\n",
    "    print(result.stdout)\n",
    "except FileNotFoundError:\n",
    "    print(\"ℹ️  Local Ollama not installed (this is fine if using Railway)\")\n",
    "    print(\"\\n💡 You're using Railway Ollama, so local installation is optional.\")\n",
    "    print(\"\\n📝 If you want to install Ollama locally for development:\")\n",
    "    print(\"   curl -fsSL https://ollama.com/install.sh | sh\")\n",
    "    print(\"   ollama pull llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Initialize FREE Models\n",
    "\n",
    "Set up our local LLM and embedding model - completely free!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading embedding model (first time may take a minute)...\n",
      "✅ Embedding model loaded on GPU!\n",
      "🚂 Using RAILWAY Ollama: http://ollama-production-4331.up.railway.app\n",
      "\n",
      "🧪 Testing llama3.2:3b...\n",
      "❌ Connection failed: 405 method not allowed (status code: 405)\n",
      "\n",
      "💡 Troubleshooting:\n",
      "   1. Railway might require TCP proxy instead of HTTP\n",
      "   2. Check Railway logs for connection errors\n",
      "   3. Try: railway logs\n",
      "   4. Verify model is running: railway ssh -> ollama list\n"
     ]
    }
   ],
   "source": [
    "# Initialize FREE embedding model\n",
    "print(\"🔄 Loading embedding model (first time may take a minute)...\")\n",
    "\n",
    "# 🎮 GPU DETECTED! Using better quality model\n",
    "# Since you have a GPU, we can use a higher quality model without speed penalty\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",  # Better quality with GPU! (420MB)\n",
    "    model_kwargs={'device': 'cuda'}  # 🚀 GPU acceleration enabled!\n",
    ")\n",
    "\n",
    "# Alternative options:\n",
    "# For even better quality (1.3GB): \"BAAI/bge-large-en-v1.5\"\n",
    "# For faster/smaller (90MB): \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(\"✅ Embedding model loaded on GPU!\")\n",
    "\n",
    "# Initialize LLM - Connect to Railway Ollama\n",
    "MODEL_NAME = \"llama3.2:3b\"  # ✅ Updated to match your installed model\n",
    "\n",
    "# 🚂 RAILWAY CONFIGURATION\n",
    "# ⚠️ Fixed: Using HTTP instead of HTTPS (Railway doesn't support SSL for Ollama)\n",
    "OLLAMA_BASE_URL = \"http://ollama-production-4331.up.railway.app\"\n",
    "\n",
    "# Toggle: Use local or Railway Ollama\n",
    "USE_LOCAL_OLLAMA = False  # Set to True to use local Ollama instead\n",
    "\n",
    "if USE_LOCAL_OLLAMA:\n",
    "    base_url = \"http://localhost:11434\"\n",
    "    print(\"🔧 Using LOCAL Ollama\")\n",
    "else:\n",
    "    base_url = OLLAMA_BASE_URL\n",
    "    print(f\"🚂 Using RAILWAY Ollama: {base_url}\")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Test the LLM\n",
    "print(f\"\\n🧪 Testing {MODEL_NAME}...\")\n",
    "try:\n",
    "    test_response = llm.invoke(\"Say 'Hello! I am ready to help with RAG.'\")\n",
    "    print(f\"✅ Response: {test_response.content}\")\n",
    "    print(\"\\n✅ Connection successful! Ready to go!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Connection failed: {e}\")\n",
    "    print(\"\\n💡 Troubleshooting:\")\n",
    "    print(\"   1. Railway might require TCP proxy instead of HTTP\")\n",
    "    print(\"   2. Check Railway logs for connection errors\")\n",
    "    print(\"   3. Try: railway logs\")\n",
    "    print(\"   4. Verify model is running: railway ssh -> ollama list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Explore PDF Documents\n",
    "\n",
    "Let's examine what PDFs we have and understand their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "PDF_DIR = Path(\"pdf_documents\")\n",
    "\n",
    "# List all PDF files\n",
    "pdf_files = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "print(f\"📂 Found {len(pdf_files)} PDF documents:\\n\")\n",
    "for i, pdf_path in enumerate(pdf_files, 1):\n",
    "    file_size = pdf_path.stat().st_size / 1024  # KB\n",
    "    print(f\"{i}. {pdf_path.name}\")\n",
    "    print(f\"   Size: {file_size:.1f} KB\\n\")\n",
    "\n",
    "# Domain mapping based on file names\n",
    "domain_mapping = {\n",
    "    \"legal\": [\"PDPA\", \"Advisory\", \"Guidelines\", \"Enforcement\"],\n",
    "    \"hr\": [\"Employee\", \"Handbook\"],\n",
    "}\n",
    "\n",
    "def classify_pdf(filename: str) -> str:\n",
    "    \"\"\"Classify PDF into domain based on filename.\"\"\"\n",
    "    for domain, keywords in domain_mapping.items():\n",
    "        if any(keyword.lower() in filename.lower() for keyword in keywords):\n",
    "            return domain\n",
    "    return \"general\"\n",
    "\n",
    "# Classify documents\n",
    "classified_docs = {}\n",
    "for pdf_path in pdf_files:\n",
    "    domain = classify_pdf(pdf_path.name)\n",
    "    if domain not in classified_docs:\n",
    "        classified_docs[domain] = []\n",
    "    classified_docs[domain].append(pdf_path)\n",
    "\n",
    "print(\"\\n📊 Domain Classification:\")\n",
    "for domain, docs in classified_docs.items():\n",
    "    print(f\"\\n{domain.upper()}: {len(docs)} document(s)\")\n",
    "    for doc in docs:\n",
    "        print(f\"  • {doc.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Load PDF Documents\n",
    "\n",
    "Now let's load the PDF documents with proper metadata tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_with_metadata(pdf_path: Path, domain: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a PDF file and add domain metadata.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        domain: Domain classification (legal, hr, general)\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects (one per page)\n",
    "    \"\"\"\n",
    "    print(f\"📄 Loading: {pdf_path.name}\")\n",
    "    \n",
    "    loader = PyPDFLoader(str(pdf_path))\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Add comprehensive metadata\n",
    "    for doc in documents:\n",
    "        doc.metadata[\"domain\"] = domain\n",
    "        doc.metadata[\"source_type\"] = \"pdf\"\n",
    "        doc.metadata[\"filename\"] = pdf_path.name\n",
    "        doc.metadata[\"file_path\"] = str(pdf_path)\n",
    "    \n",
    "    print(f\"   ✓ Loaded {len(documents)} pages\")\n",
    "    return documents\n",
    "\n",
    "# Load all documents by domain\n",
    "all_documents = {}\n",
    "document_stats = {}\n",
    "\n",
    "print(\"🔄 Loading all PDF documents...\\n\")\n",
    "\n",
    "for domain, pdf_paths in classified_docs.items():\n",
    "    domain_docs = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        docs = load_pdf_with_metadata(pdf_path, domain)\n",
    "        domain_docs.extend(docs)\n",
    "    \n",
    "    all_documents[domain] = domain_docs\n",
    "    document_stats[domain] = {\n",
    "        \"num_files\": len(pdf_paths),\n",
    "        \"num_pages\": len(domain_docs),\n",
    "        \"total_chars\": sum(len(doc.page_content) for doc in domain_docs)\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📈 DOCUMENT LOADING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for domain, stats in document_stats.items():\n",
    "    print(f\"\\n{domain.upper()}:\")\n",
    "    print(f\"  Files: {stats['num_files']}\")\n",
    "    print(f\"  Pages: {stats['num_pages']}\")\n",
    "    print(f\"  Characters: {stats['total_chars']:,}\")\n",
    "    print(f\"  Avg chars/page: {stats['total_chars']//stats['num_pages']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Document Chunking Strategies\n",
    "\n",
    "PDF pages can be very long. We'll split them into smaller chunks for better retrieval.\n",
    "\n",
    "### 📝 Chunking Parameters:\n",
    "- **chunk_size**: Maximum characters per chunk (1000)\n",
    "- **chunk_overlap**: Characters shared between chunks (200)\n",
    "- **separators**: Split at paragraphs, then sentences, then words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunked_documents = {}\n",
    "chunk_stats = {}\n",
    "\n",
    "print(\"✂️ Splitting documents into chunks...\\n\")\n",
    "\n",
    "for domain, docs in all_documents.items():\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    chunked_documents[domain] = chunks\n",
    "    \n",
    "    chunk_stats[domain] = {\n",
    "        \"num_chunks\": len(chunks),\n",
    "        \"avg_chunk_size\": sum(len(c.page_content) for c in chunks) / len(chunks) if chunks else 0,\n",
    "        \"min_chunk_size\": min(len(c.page_content) for c in chunks) if chunks else 0,\n",
    "        \"max_chunk_size\": max(len(c.page_content) for c in chunks) if chunks else 0,\n",
    "    }\n",
    "    \n",
    "    print(f\"{domain.upper()}: {len(docs)} pages → {len(chunks)} chunks\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 CHUNKING STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for domain, stats in chunk_stats.items():\n",
    "    print(f\"\\n{domain.upper()}:\")\n",
    "    print(f\"  Total chunks: {stats['num_chunks']}\")\n",
    "    print(f\"  Avg size: {stats['avg_chunk_size']:.0f} chars\")\n",
    "    print(f\"  Range: {stats['min_chunk_size']:.0f} - {stats['max_chunk_size']:.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 Visualize Chunk Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of chunk sizes\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=len(chunked_documents),\n",
    "    subplot_titles=[f\"{domain.upper()}\" for domain in chunked_documents.keys()],\n",
    "    specs=[[{\"type\": \"histogram\"}] * len(chunked_documents)]\n",
    ")\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#95E1D3', '#F38181']\n",
    "for i, (domain, chunks) in enumerate(chunked_documents.items(), 1):\n",
    "    chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=chunk_sizes,\n",
    "            name=domain.upper(),\n",
    "            nbinsx=20,\n",
    "            marker_color=colors[i-1] if i <= len(colors) else '#999999'\n",
    "        ),\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Chunk Size Distribution by Domain\",\n",
    "    showlegend=False,\n",
    "    height=400\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Chunk Size (characters)\")\n",
    "fig.update_yaxes(title_text=\"Frequency\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n💡 Insight: Consistent chunk sizes (around 800-1000 chars) ensure balanced retrieval across domains.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Create Vector Indexes\n",
    "\n",
    "We'll create separate vector stores for each domain using ChromaDB and FREE embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔄 Creating vector indexes with FREE embeddings...\\n\")\n",
    "\n",
    "# Create vector stores\n",
    "vectorstores = {}\n",
    "retrievers = {}\n",
    "\n",
    "for domain, chunks in chunked_documents.items():\n",
    "    if not chunks:\n",
    "        print(f\"⚠️ Skipping {domain} - no chunks available\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"📦 Creating {domain.upper()} vector store...\")\n",
    "    \n",
    "    # Create vector store with persistence\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=f\"{domain}_docs\",\n",
    "        persist_directory=f\"./chroma_db_free/{domain}\"\n",
    "    )\n",
    "    \n",
    "    vectorstores[domain] = vectorstore\n",
    "    retrievers[domain] = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 3}  # Retrieve top 3 chunks\n",
    "    )\n",
    "    \n",
    "    print(f\"   ✓ Indexed {len(chunks)} chunks\")\n",
    "\n",
    "print(f\"\\n✅ Created {len(vectorstores)} domain-specific indexes!\")\n",
    "print(f\"📁 Vector stores persisted to: ./chroma_db_free/\")\n",
    "print(\"\\n💰 Cost: $0.00 - Everything runs locally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: LLM-Based Query Router (FREE)\n",
    "\n",
    "The router decides which domain index to query based on the question.\n",
    "Using JSON output parsing since Ollama doesn't support function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define routing schema\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant domain-specific index.\"\"\"\n",
    "    \n",
    "    datasource: Literal[\"legal\", \"hr\", \"general\"] = Field(\n",
    "        ...,\n",
    "        description=\"Choose the most relevant datasource for the query\"\n",
    "    )\n",
    "    \n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"Brief explanation of why this datasource was chosen\"\n",
    "    )\n",
    "\n",
    "# Create routing prompt (JSON-based for Ollama)\n",
    "routing_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a query router. Analyze the question and route it to the most relevant datasource.\n",
    "\n",
    "Available datasources:\n",
    "- legal: Data privacy, PDPA, enforcement guidelines, compliance, personal data protection, legal regulations\n",
    "- hr: Employee handbook, workplace policies, HR procedures, benefits, employee conduct, workplace rules\n",
    "- general: Questions that don't fit legal or HR categories\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Respond with ONLY a JSON object in this exact format:\n",
    "{{\n",
    "    \"datasource\": \"legal\" or \"hr\" or \"general\",\n",
    "    \"reasoning\": \"brief explanation\"\n",
    "}}\n",
    "\n",
    "JSON Response:\"\"\")\n",
    "\n",
    "# Create router chain\n",
    "router_chain = routing_prompt | llm | JsonOutputParser()\n",
    "\n",
    "def route_query(question: str) -> str:\n",
    "    \"\"\"Route a query to the appropriate domain.\"\"\"\n",
    "    try:\n",
    "        result = router_chain.invoke({\"question\": question})\n",
    "        datasource = result.get(\"datasource\", \"general\")\n",
    "        reasoning = result.get(\"reasoning\", \"No reasoning provided\")\n",
    "        return datasource, reasoning\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Routing error: {e}\")\n",
    "        return \"general\", \"Fallback due to error\"\n",
    "\n",
    "print(\"✅ Router configured successfully!\")\n",
    "print(\"\\n🔀 Available routes:\")\n",
    "for route in retrievers.keys():\n",
    "    print(f\"  • {route.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test routing with sample queries\n",
    "test_routing_queries = [\n",
    "    \"What are the penalties for PDPA violations?\",\n",
    "    \"What is the company's vacation policy?\",\n",
    "    \"How should personal data be collected?\",\n",
    "    \"What are the dress code requirements?\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing Router with Sample Queries\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "routing_results = []\n",
    "for query in test_routing_queries:\n",
    "    route, reasoning = route_query(query)\n",
    "    routing_results.append({\"query\": query, \"route\": route, \"reasoning\": reasoning})\n",
    "    print(f\"❓ {query}\")\n",
    "    print(f\"   ➜ Routed to: {route.upper()}\")\n",
    "    print(f\"   💭 Reasoning: {reasoning}\\n\")\n",
    "\n",
    "print(\"✅ Router test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Complete Multi-Index RAG Pipeline (FREE)\n",
    "\n",
    "Now let's build the complete pipeline with citation tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant answering questions based on PDF documents.\n",
    "\n",
    "Context from PDF documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Provide a detailed, accurate answer based ONLY on the information in the PDF documents above\n",
    "2. If the answer isn't in the documents, clearly state that\n",
    "3. Be specific and cite relevant details from the documents\n",
    "4. Keep the answer concise but comprehensive\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "def format_docs_with_citations(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents with source information for citations.\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get(\"filename\", \"Unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        formatted.append(\n",
    "            f\"[Source {i}: {source}, Page {page}]\\n{doc.page_content}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "def multi_index_rag(question: str, verbose: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete Multi-Index RAG pipeline with routing, retrieval, and generation.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        verbose: Print detailed information\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with question, route, answer, sources, and metrics\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"❓ Question: {question}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    # Step 1: Route query\n",
    "    route_start = time.time()\n",
    "    selected_route, reasoning = route_query(question)\n",
    "    route_time = time.time() - route_start\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n🔀 Routing Decision: {selected_route.upper()}\")\n",
    "        print(f\"   💭 Reasoning: {reasoning}\")\n",
    "        print(f\"   ⏱️ Time: {route_time:.3f}s\")\n",
    "    \n",
    "    # Step 2: Retrieve from selected index\n",
    "    retrieval_start = time.time()\n",
    "    selected_retriever = retrievers.get(selected_route, list(retrievers.values())[0])\n",
    "    retrieved_docs = selected_retriever.invoke(question)\n",
    "    retrieval_time = time.time() - retrieval_start\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n📚 Retrieved {len(retrieved_docs)} relevant chunks\")\n",
    "        print(f\"   ⏱️ Time: {retrieval_time:.3f}s\")\n",
    "        print(f\"\\n📄 Sources:\")\n",
    "        for doc in retrieved_docs:\n",
    "            source = doc.metadata.get(\"filename\", \"Unknown\")\n",
    "            page = doc.metadata.get(\"page\", \"?\")\n",
    "            preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "            print(f\"   • {source} (Page {page})\")\n",
    "            print(f\"     Preview: {preview}...\")\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    generation_start = time.time()\n",
    "    context = format_docs_with_citations(retrieved_docs)\n",
    "    rag_chain = rag_prompt | llm | StrOutputParser()\n",
    "    answer = rag_chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    })\n",
    "    generation_time = time.time() - generation_start\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n💡 Answer:\\n{answer}\")\n",
    "        print(f\"\\n⏱️ Performance:\")\n",
    "        print(f\"   Routing: {route_time:.3f}s\")\n",
    "        print(f\"   Retrieval: {retrieval_time:.3f}s\")\n",
    "        print(f\"   Generation: {generation_time:.3f}s\")\n",
    "        print(f\"   Total: {total_time:.3f}s\")\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"route\": selected_route,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"filename\": doc.metadata.get(\"filename\"),\n",
    "                \"page\": doc.metadata.get(\"page\"),\n",
    "                \"content_preview\": doc.page_content[:200]\n",
    "            }\n",
    "            for doc in retrieved_docs\n",
    "        ],\n",
    "        \"metrics\": {\n",
    "            \"route_time\": route_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": total_time,\n",
    "            \"num_chunks_retrieved\": len(retrieved_docs)\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"✅ RAG pipeline ready!\")\n",
    "print(\"💰 Cost: $0.00 - Everything runs locally with Ollama!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Test with Real Queries\n",
    "\n",
    "Let's test the system with questions relevant to our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions\n",
    "test_questions = [\n",
    "    # Legal/PDPA questions\n",
    "    \"What are the key obligations for organizations under PDPA?\",\n",
    "    \"What are the penalties for data protection violations?\",\n",
    "    \n",
    "    # HR/Employee Handbook questions\n",
    "    \"What are the employee benefits mentioned in the handbook?\",\n",
    "    \"What is the policy on working hours?\",\n",
    "]\n",
    "\n",
    "# Run queries and collect results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🧪 TESTING MULTI-INDEX RAG SYSTEM (FREE VERSION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = []\n",
    "for question in test_questions:\n",
    "    result = multi_index_rag(question, verbose=True)\n",
    "    all_results.append(result)\n",
    "    print(\"\\n\" + \"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Visualize Routing Decisions\n",
    "\n",
    "Let's visualize how queries were routed across domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze routing distribution\n",
    "routing_distribution = {}\n",
    "for result in all_results:\n",
    "    route = result[\"route\"]\n",
    "    routing_distribution[route] = routing_distribution.get(route, 0) + 1\n",
    "\n",
    "# Create visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(routing_distribution.keys()),\n",
    "    y=list(routing_distribution.values()),\n",
    "    marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1'],\n",
    "    text=list(routing_distribution.values()),\n",
    "    textposition='auto',\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Query Routing Distribution (Free Local Models)\",\n",
    "    xaxis_title=\"Domain\",\n",
    "    yaxis_title=\"Number of Queries\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Create routing flow diagram\n",
    "print(\"\\n📊 Routing Summary:\")\n",
    "for route, count in routing_distribution.items():\n",
    "    percentage = (count / len(all_results)) * 100\n",
    "    print(f\"  {route.upper()}: {count} queries ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 11: Performance Metrics & Evaluation\n",
    "\n",
    "Let's analyze the system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performance metrics\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Question\": result[\"question\"][:40] + \"...\",\n",
    "        \"Route\": result[\"route\"],\n",
    "        \"Route (s)\": result[\"metrics\"][\"route_time\"],\n",
    "        \"Retrieval (s)\": result[\"metrics\"][\"retrieval_time\"],\n",
    "        \"Generation (s)\": result[\"metrics\"][\"generation_time\"],\n",
    "        \"Total (s)\": result[\"metrics\"][\"total_time\"],\n",
    "        \"Chunks\": result[\"metrics\"][\"num_chunks_retrieved\"]\n",
    "    }\n",
    "    for result in all_results\n",
    "])\n",
    "\n",
    "print(\"\\n📈 PERFORMANCE METRICS (FREE LOCAL MODELS)\")\n",
    "print(\"=\"*70)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"\\n📊 SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Average Total Time: {metrics_df['Total (s)'].mean():.3f}s\")\n",
    "print(f\"Average Route Time: {metrics_df['Route (s)'].mean():.3f}s\")\n",
    "print(f\"Average Retrieval Time: {metrics_df['Retrieval (s)'].mean():.3f}s\")\n",
    "print(f\"Average Generation Time: {metrics_df['Generation (s)'].mean():.3f}s\")\n",
    "print(f\"\\n💰 Total Cost: $0.00 (vs ~$0.10-0.50 with OpenAI)\")\n",
    "\n",
    "# Visualize time breakdown\n",
    "time_components = [\n",
    "    metrics_df['Route (s)'].mean(),\n",
    "    metrics_df['Retrieval (s)'].mean(),\n",
    "    metrics_df['Generation (s)'].mean()\n",
    "]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    labels=['Routing', 'Retrieval', 'Generation'],\n",
    "    values=time_components,\n",
    "    marker_colors=['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Average Time Distribution - Free Local Models\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 12: Interactive Testing Section\n",
    "\n",
    "Now it's your turn! Try asking your own questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query function\n",
    "def ask_question(question: str):\n",
    "    \"\"\"Ask a question and get an answer with full tracking.\"\"\"\n",
    "    result = multi_index_rag(question, verbose=True)\n",
    "    return result\n",
    "\n",
    "# Example: Try your own question!\n",
    "# Uncomment and modify the question below:\n",
    "\n",
    "# my_question = \"What are data breach notification requirements?\"\n",
    "# my_result = ask_question(my_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 13: Save Results\n",
    "\n",
    "Let's save our results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Save results to JSON\n",
    "results_file = f\"rag_results_free_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "output_data = {\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"num_domains\": len(vectorstores),\n",
    "        \"domains\": list(vectorstores.keys()),\n",
    "        \"total_queries\": len(all_results),\n",
    "        \"cost\": \"$0.00 (Free Local Models)\"\n",
    "    },\n",
    "    \"document_stats\": document_stats,\n",
    "    \"chunk_stats\": chunk_stats,\n",
    "    \"results\": all_results,\n",
    "    \"performance_summary\": {\n",
    "        \"avg_total_time\": metrics_df['Total (s)'].mean(),\n",
    "        \"avg_route_time\": metrics_df['Route (s)'].mean(),\n",
    "        \"avg_retrieval_time\": metrics_df['Retrieval (s)'].mean(),\n",
    "        \"avg_generation_time\": metrics_df['Generation (s)'].mean(),\n",
    "    },\n",
    "    \"routing_distribution\": routing_distribution\n",
    "}\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Results saved to: {results_file}\")\n",
    "print(f\"\\n📊 Session Summary:\")\n",
    "print(f\"  • Model: {MODEL_NAME} (Free Local)\")\n",
    "print(f\"  • Embeddings: sentence-transformers/all-MiniLM-L6-v2 (Free)\")\n",
    "print(f\"  • Processed {sum(stats['num_files'] for stats in document_stats.values())} PDF files\")\n",
    "print(f\"  • Created {len(vectorstores)} domain indexes\")\n",
    "print(f\"  • Answered {len(all_results)} queries\")\n",
    "print(f\"  • Average response time: {metrics_df['Total (s)'].mean():.3f}s\")\n",
    "print(f\"  • 💰 Total cost: $0.00\")\n",
    "\n",
    "print(\"\\n🎉 Tutorial Complete! You've mastered Multi-Index RAG with FREE local models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 💡 Key Advantages of Free Local Models\n",
    "\n",
    "### ✅ Pros:\n",
    "1. **Zero Cost** - No API fees, run unlimited queries\n",
    "2. **Privacy** - Data never leaves your machine\n",
    "3. **No Rate Limits** - Process as much as you want\n",
    "4. **Offline Capable** - Works without internet\n",
    "5. **Customizable** - Fine-tune models for your domain\n",
    "\n",
    "### ⚠️ Considerations:\n",
    "1. **Speed** - Local models may be slower (especially on CPU)\n",
    "2. **Quality** - Smaller models may produce less accurate results\n",
    "3. **Setup** - Requires Ollama installation\n",
    "4. **Hardware** - Better with GPU, but works on CPU\n",
    "\n",
    "### 🚀 Performance Tips:\n",
    "1. **Use GPU** - If available, configure HuggingFaceEmbeddings with `device='cuda'`\n",
    "2. **Choose Right Model**:\n",
    "   - `phi3` - Fastest, smallest (2GB)\n",
    "   - `llama3.2` - Good balance (2GB)\n",
    "   - `mistral` - Best quality (4GB)\n",
    "3. **Optimize Chunk Size** - Smaller chunks (500-800) work better with smaller models\n",
    "4. **Batch Processing** - Process multiple queries together\n",
    "\n",
    "### 🔄 Switching Models:\n",
    "```python\n",
    "# Try different models:\n",
    "MODEL_NAME = \"phi3\"          # Fastest\n",
    "MODEL_NAME = \"llama3.2\"      # Balanced  \n",
    "MODEL_NAME = \"mistral\"       # Best quality\n",
    "MODEL_NAME = \"gemma2\"        # Google's model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎯 Practice Exercises\n",
    "\n",
    "### Exercise 1: Try Different Models\n",
    "1. Install multiple Ollama models: `ollama pull phi3`, `ollama pull mistral`\n",
    "2. Run the same queries with different models\n",
    "3. Compare speed and quality\n",
    "4. Document which model works best for your use case\n",
    "\n",
    "### Exercise 2: Optimize for Speed\n",
    "1. Reduce chunk size to 500 characters\n",
    "2. Retrieve fewer chunks (k=2)\n",
    "3. Use a faster model (phi3)\n",
    "4. Measure performance improvement\n",
    "\n",
    "### Exercise 3: Add New Documents\n",
    "1. Add more PDFs to `pdf_documents/`\n",
    "2. Create a new domain category\n",
    "3. Update the routing logic\n",
    "4. Test with new queries\n",
    "\n",
    "### Exercise 4: Build a Web Interface\n",
    "1. Create a Streamlit app for the RAG system\n",
    "2. Add file upload capability\n",
    "3. Display routing decisions visually\n",
    "4. Show source citations with links\n",
    "\n",
    "### Exercise 5: Hybrid Approach\n",
    "1. Keep embeddings local (HuggingFace)\n",
    "2. Use OpenAI for generation only (fallback)\n",
    "3. Compare costs and performance\n",
    "4. Find the optimal balance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
