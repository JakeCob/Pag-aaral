{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Index Routing RAG with PDF Documents - Interactive Learning Notebook\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "In this notebook, you will learn:\n",
    "\n",
    "1. **Multi-Index RAG Architecture** - How to route queries to domain-specific indexes\n",
    "2. **PDF Processing** - Load and process real PDF documents\n",
    "3. **Document Chunking Strategies** - Split documents effectively for retrieval\n",
    "4. **LLM-Based Routing** - Use AI to intelligently route queries\n",
    "5. **Citation Tracking** - Track sources and page numbers for answers\n",
    "6. **Evaluation Metrics** - Measure system performance\n",
    "\n",
    "## üìö Dataset\n",
    "\n",
    "We'll process 4 PDF documents from `pdf_documents/` folder:\n",
    "- **Legal/Compliance Domain**: 3 PDPA Advisory Guidelines\n",
    "- **HR/Business Domain**: 1 Employee Handbook\n",
    "- **General Domain**: Fallback for other queries\n",
    "\n",
    "## üîß Prerequisites\n",
    "\n",
    "Make sure you have:\n",
    "- OpenAI API key set as environment variable: `OPENAI_API_KEY`\n",
    "- Python 3.8+\n",
    "- Required packages (we'll install in next cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Setup and Dependencies\n",
    "\n",
    "First, let's install and import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -q openai langchain langchain-openai langchain-community chromadb pypdf python-dotenv matplotlib plotly pandas"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Literal, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain_core.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"‚ö†Ô∏è OPENAI_API_KEY not found in environment variables!\")\n",
    "\n",
    "print(\"‚úÖ All dependencies loaded successfully!\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Explore PDF Documents\n",
    "\n",
    "Let's examine what PDFs we have and understand their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define paths\n",
    "PDF_DIR = Path(\"pdf_documents\")\n",
    "\n",
    "# List all PDF files\n",
    "pdf_files = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "print(f\"üìÇ Found {len(pdf_files)} PDF documents:\\n\")\n",
    "for i, pdf_path in enumerate(pdf_files, 1):\n",
    "    file_size = pdf_path.stat().st_size / 1024  # KB\n",
    "    print(f\"{i}. {pdf_path.name}\")\n",
    "    print(f\"   Size: {file_size:.1f} KB\\n\")\n",
    "\n",
    "# Domain mapping based on file names\n",
    "domain_mapping = {\n",
    "    \"legal\": [\"PDPA\", \"Advisory\", \"Guidelines\", \"Enforcement\"],\n",
    "    \"hr\": [\"Employee\", \"Handbook\"],\n",
    "}\n",
    "\n",
    "def classify_pdf(filename: str) -> str:\n",
    "    \"\"\"Classify PDF into domain based on filename.\"\"\"\n",
    "    for domain, keywords in domain_mapping.items():\n",
    "        if any(keyword.lower() in filename.lower() for keyword in keywords):\n",
    "            return domain\n",
    "    return \"general\"\n",
    "\n",
    "# Classify documents\n",
    "classified_docs = {}\n",
    "for pdf_path in pdf_files:\n",
    "    domain = classify_pdf(pdf_path.name)\n",
    "    if domain not in classified_docs:\n",
    "        classified_docs[domain] = []\n",
    "    classified_docs[domain].append(pdf_path)\n",
    "\n",
    "print(\"\\nüìä Domain Classification:\")\n",
    "for domain, docs in classified_docs.items():\n",
    "    print(f\"\\n{domain.upper()}: {len(docs)} document(s)\")\n",
    "    for doc in docs:\n",
    "        print(f\"  ‚Ä¢ {doc.name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Load PDF Documents\n",
    "\n",
    "Now let's load the PDF documents with proper metadata tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_pdf_with_metadata(pdf_path: Path, domain: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a PDF file and add domain metadata.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        domain: Domain classification (legal, hr, general)\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects (one per page)\n",
    "    \"\"\"\n",
    "    print(f\"üìÑ Loading: {pdf_path.name}\")\n",
    "    \n",
    "    loader = PyPDFLoader(str(pdf_path))\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Add comprehensive metadata\n",
    "    for doc in documents:\n",
    "        doc.metadata[\"domain\"] = domain\n",
    "        doc.metadata[\"source_type\"] = \"pdf\"\n",
    "        doc.metadata[\"filename\"] = pdf_path.name\n",
    "        doc.metadata[\"file_path\"] = str(pdf_path)\n",
    "    \n",
    "    print(f\"   ‚úì Loaded {len(documents)} pages\")\n",
    "    return documents\n",
    "\n",
    "# Load all documents by domain\n",
    "all_documents = {}\n",
    "document_stats = {}\n",
    "\n",
    "print(\"üîÑ Loading all PDF documents...\\n\")\n",
    "\n",
    "for domain, pdf_paths in classified_docs.items():\n",
    "    domain_docs = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        docs = load_pdf_with_metadata(pdf_path, domain)\n",
    "        domain_docs.extend(docs)\n",
    "    \n",
    "    all_documents[domain] = domain_docs\n",
    "    document_stats[domain] = {\n",
    "        \"num_files\": len(pdf_paths),\n",
    "        \"num_pages\": len(domain_docs),\n",
    "        \"total_chars\": sum(len(doc.page_content) for doc in domain_docs)\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà DOCUMENT LOADING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for domain, stats in document_stats.items():\n",
    "    print(f\"\\n{domain.upper()}:\")\n",
    "    print(f\"  Files: {stats['num_files']}\")\n",
    "    print(f\"  Pages: {stats['num_pages']}\")\n",
    "    print(f\"  Characters: {stats['total_chars']:,}\")\n",
    "    print(f\"  Avg chars/page: {stats['total_chars']//stats['num_pages']:,}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Document Chunking Strategies\n",
    "\n",
    "PDF pages can be very long. We'll split them into smaller chunks for better retrieval.\n",
    "\n",
    "### üìù Chunking Parameters:\n",
    "- **chunk_size**: Maximum characters per chunk (1000)\n",
    "- **chunk_overlap**: Characters shared between chunks (200)\n",
    "- **separators**: Split at paragraphs, then sentences, then words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunked_documents = {}\n",
    "chunk_stats = {}\n",
    "\n",
    "print(\"‚úÇÔ∏è Splitting documents into chunks...\\n\")\n",
    "\n",
    "for domain, docs in all_documents.items():\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    chunked_documents[domain] = chunks\n",
    "    \n",
    "    chunk_stats[domain] = {\n",
    "        \"num_chunks\": len(chunks),\n",
    "        \"avg_chunk_size\": sum(len(c.page_content) for c in chunks) / len(chunks) if chunks else 0,\n",
    "        \"min_chunk_size\": min(len(c.page_content) for c in chunks) if chunks else 0,\n",
    "        \"max_chunk_size\": max(len(c.page_content) for c in chunks) if chunks else 0,\n",
    "    }\n",
    "    \n",
    "    print(f\"{domain.upper()}: {len(docs)} pages ‚Üí {len(chunks)} chunks\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä CHUNKING STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for domain, stats in chunk_stats.items():\n",
    "    print(f\"\\n{domain.upper()}:\")\n",
    "    print(f\"  Total chunks: {stats['num_chunks']}\")\n",
    "    print(f\"  Avg size: {stats['avg_chunk_size']:.0f} chars\")\n",
    "    print(f\"  Range: {stats['min_chunk_size']:.0f} - {stats['max_chunk_size']:.0f} chars\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualize Chunk Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create visualization of chunk sizes\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=len(chunked_documents),\n",
    "    subplot_titles=[f\"{domain.upper()}\" for domain in chunked_documents.keys()],\n",
    "    specs=[[{\"type\": \"histogram\"}] * len(chunked_documents)]\n",
    ")\n",
    "\n",
    "for i, (domain, chunks) in enumerate(chunked_documents.items(), 1):\n",
    "    chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=chunk_sizes,\n",
    "            name=domain.upper(),\n",
    "            nbinsx=20,\n",
    "            marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1'][i-1]\n",
    "        ),\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Chunk Size Distribution by Domain\",\n",
    "    showlegend=False,\n",
    "    height=400\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Chunk Size (characters)\")\n",
    "fig.update_yaxes(title_text=\"Frequency\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüí° Insight: Consistent chunk sizes (around 800-1000 chars) ensure balanced retrieval across domains.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Create Vector Indexes\n",
    "\n",
    "We'll create separate vector stores for each domain using ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"üîÑ Creating vector indexes...\\n\")\n",
    "\n",
    "# Create vector stores\n",
    "vectorstores = {}\n",
    "retrievers = {}\n",
    "\n",
    "for domain, chunks in chunked_documents.items():\n",
    "    if not chunks:\n",
    "        print(f\"‚ö†Ô∏è Skipping {domain} - no chunks available\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"üì¶ Creating {domain.upper()} vector store...\")\n",
    "    \n",
    "    # Create vector store with persistence\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=f\"{domain}_docs\",\n",
    "        persist_directory=f\"./chroma_db/{domain}\"\n",
    "    )\n",
    "    \n",
    "    vectorstores[domain] = vectorstore\n",
    "    retrievers[domain] = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 3}  # Retrieve top 3 chunks\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úì Indexed {len(chunks)} chunks\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(vectorstores)} domain-specific indexes!\")\n",
    "print(f\"üìÅ Vector stores persisted to: ./chroma_db/\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: LLM-Based Query Router\n",
    "\n",
    "The router decides which domain index to query based on the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define routing schema\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant domain-specific index.\"\"\"\n",
    "    \n",
    "    datasource: Literal[\"legal\", \"hr\", \"general\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"\n",
    "        Choose the most relevant datasource for the query:\n",
    "        - legal: Data privacy, PDPA, enforcement guidelines, compliance, personal data protection\n",
    "        - hr: Employee handbook, workplace policies, HR procedures, benefits, employee conduct\n",
    "        - general: Questions that don't fit legal or HR categories\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    confidence: float = Field(\n",
    "        ...,\n",
    "        description=\"Confidence score between 0 and 1 for this routing decision\",\n",
    "        ge=0.0,\n",
    "        le=1.0\n",
    "    )\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create router chain\n",
    "router_function = convert_pydantic_to_openai_function(RouteQuery)\n",
    "router_chain = (\n",
    "    llm.bind(\n",
    "        functions=[router_function],\n",
    "        function_call={\"name\": \"RouteQuery\"}\n",
    "    )\n",
    "    | PydanticAttrOutputFunctionsParser(\n",
    "        pydantic_schema=RouteQuery,\n",
    "        attr_name=\"datasource\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Router configured successfully!\")\n",
    "print(\"\\nüîÄ Available routes:\")\n",
    "for route in retrievers.keys():\n",
    "    print(f\"  ‚Ä¢ {route.upper()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test routing with sample queries\n",
    "test_routing_queries = [\n",
    "    \"What are the penalties for PDPA violations?\",\n",
    "    \"What is the company's vacation policy?\",\n",
    "    \"How should personal data be collected?\",\n",
    "    \"What are the dress code requirements?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Router with Sample Queries\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "routing_results = []\n",
    "for query in test_routing_queries:\n",
    "    route = router_chain.invoke({\"question\": query})\n",
    "    routing_results.append({\"query\": query, \"route\": route})\n",
    "    print(f\"‚ùì {query}\")\n",
    "    print(f\"   ‚ûú Routed to: {route.upper()}\\n\")\n",
    "\n",
    "print(\"‚úÖ Router test complete!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Complete Multi-Index RAG Pipeline\n",
    "\n",
    "Now let's build the complete pipeline with citation tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant answering questions based on PDF documents.\n",
    "\n",
    "Context from PDF documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Provide a detailed, accurate answer based ONLY on the information in the PDF documents above\n",
    "2. If the answer isn't in the documents, clearly state that\n",
    "3. Be specific and cite relevant details from the documents\n",
    "4. Keep the answer concise but comprehensive\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "def format_docs_with_citations(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents with source information for citations.\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get(\"filename\", \"Unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        formatted.append(\n",
    "            f\"[Source {i}: {source}, Page {page}]\\n{doc.page_content}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "def multi_index_rag(question: str, verbose: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete Multi-Index RAG pipeline with routing, retrieval, and generation.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        verbose: Print detailed information\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with question, route, answer, sources, and metrics\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"‚ùì Question: {question}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    # Step 1: Route query\n",
    "    route_start = time.time()\n",
    "    selected_route = router_chain.invoke({\"question\": question})\n",
    "    route_time = time.time() - route_start\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüîÄ Routing Decision: {selected_route.upper()}\")\n",
    "        print(f\"   Time: {route_time:.3f}s\")\n",
    "    \n",
    "    # Step 2: Retrieve from selected index\n",
    "    retrieval_start = time.time()\n",
    "    selected_retriever = retrievers[selected_route]\n",
    "    retrieved_docs = selected_retriever.invoke(question)\n",
    "    retrieval_time = time.time() - retrieval_start\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüìö Retrieved {len(retrieved_docs)} relevant chunks\")\n",
    "        print(f\"   Time: {retrieval_time:.3f}s\")\n",
    "        print(f\"\\nüìÑ Sources:\")\n",
    "        for doc in retrieved_docs:\n",
    "            source = doc.metadata.get(\"filename\", \"Unknown\")\n",
    "            page = doc.metadata.get(\"page\", \"?\")\n",
    "            preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "            print(f\"   ‚Ä¢ {source} (Page {page})\")\n",
    "            print(f\"     Preview: {preview}...\")\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    generation_start = time.time()\n",
    "    context = format_docs_with_citations(retrieved_docs)\n",
    "    rag_chain = rag_prompt | llm | StrOutputParser()\n",
    "    answer = rag_chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    })\n",
    "    generation_time = time.time() - generation_start\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüí° Answer:\\n{answer}\")\n",
    "        print(f\"\\n‚è±Ô∏è Performance:\")\n",
    "        print(f\"   Routing: {route_time:.3f}s\")\n",
    "        print(f\"   Retrieval: {retrieval_time:.3f}s\")\n",
    "        print(f\"   Generation: {generation_time:.3f}s\")\n",
    "        print(f\"   Total: {total_time:.3f}s\")\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"route\": selected_route,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"filename\": doc.metadata.get(\"filename\"),\n",
    "                \"page\": doc.metadata.get(\"page\"),\n",
    "                \"content_preview\": doc.page_content[:200]\n",
    "            }\n",
    "            for doc in retrieved_docs\n",
    "        ],\n",
    "        \"metrics\": {\n",
    "            \"route_time\": route_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": total_time,\n",
    "            \"num_chunks_retrieved\": len(retrieved_docs)\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ RAG pipeline ready!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Test with Real Queries\n",
    "\n",
    "Let's test the system with questions relevant to our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define test questions\n",
    "test_questions = [\n",
    "    # Legal/PDPA questions\n",
    "    \"What are the key obligations for organizations under PDPA?\",\n",
    "    \"What are the penalties for data protection violations?\",\n",
    "    \"How should consent be obtained for collecting personal data?\",\n",
    "    \n",
    "    # HR/Employee Handbook questions\n",
    "    \"What are the employee benefits mentioned in the handbook?\",\n",
    "    \"What is the policy on working hours and overtime?\",\n",
    "    \"What are the grounds for employee termination?\",\n",
    "]\n",
    "\n",
    "# Run queries and collect results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ TESTING MULTI-INDEX RAG SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = []\n",
    "for question in test_questions:\n",
    "    result = multi_index_rag(question, verbose=True)\n",
    "    all_results.append(result)\n",
    "    print(\"\\n\" + \"-\"*70)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Visualize Routing Decisions\n",
    "\n",
    "Let's visualize how queries were routed across domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze routing distribution\n",
    "routing_distribution = {}\n",
    "for result in all_results:\n",
    "    route = result[\"route\"]\n",
    "    routing_distribution[route] = routing_distribution.get(route, 0) + 1\n",
    "\n",
    "# Create visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(routing_distribution.keys()),\n",
    "    y=list(routing_distribution.values()),\n",
    "    marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1'],\n",
    "    text=list(routing_distribution.values()),\n",
    "    textposition='auto',\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Query Routing Distribution\",\n",
    "    xaxis_title=\"Domain\",\n",
    "    yaxis_title=\"Number of Queries\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Create routing flow diagram\n",
    "print(\"\\nüìä Routing Summary:\")\n",
    "for route, count in routing_distribution.items():\n",
    "    percentage = (count / len(all_results)) * 100\n",
    "    print(f\"  {route.upper()}: {count} queries ({percentage:.1f}%)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Performance Metrics & Evaluation\n",
    "\n",
    "Let's analyze the system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract performance metrics\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Question\": result[\"question\"][:50] + \"...\",\n",
    "        \"Route\": result[\"route\"],\n",
    "        \"Route Time (s)\": result[\"metrics\"][\"route_time\"],\n",
    "        \"Retrieval Time (s)\": result[\"metrics\"][\"retrieval_time\"],\n",
    "        \"Generation Time (s)\": result[\"metrics\"][\"generation_time\"],\n",
    "        \"Total Time (s)\": result[\"metrics\"][\"total_time\"],\n",
    "        \"Chunks Retrieved\": result[\"metrics\"][\"num_chunks_retrieved\"]\n",
    "    }\n",
    "    for result in all_results\n",
    "])\n",
    "\n",
    "print(\"\\nüìà PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"\\nüìä SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Average Total Time: {metrics_df['Total Time (s)'].mean():.3f}s\")\n",
    "print(f\"Average Route Time: {metrics_df['Route Time (s)'].mean():.3f}s\")\n",
    "print(f\"Average Retrieval Time: {metrics_df['Retrieval Time (s)'].mean():.3f}s\")\n",
    "print(f\"Average Generation Time: {metrics_df['Generation Time (s)'].mean():.3f}s\")\n",
    "\n",
    "# Visualize time breakdown\n",
    "time_components = [\n",
    "    metrics_df['Route Time (s)'].mean(),\n",
    "    metrics_df['Retrieval Time (s)'].mean(),\n",
    "    metrics_df['Generation Time (s)'].mean()\n",
    "]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    labels=['Routing', 'Retrieval', 'Generation'],\n",
    "    values=time_components,\n",
    "    marker_colors=['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Average Time Distribution by Pipeline Stage\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 11: Interactive Testing Section\n",
    "\n",
    "Now it's your turn! Try asking your own questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Interactive query function\n",
    "def ask_question(question: str):\n",
    "    \"\"\"Ask a question and get an answer with full tracking.\"\"\"\n",
    "    result = multi_index_rag(question, verbose=True)\n",
    "    return result\n",
    "\n",
    "# Example: Try your own question!\n",
    "# Uncomment and modify the question below:\n",
    "\n",
    "# my_question = \"What are data breach notification requirements?\"\n",
    "# my_result = ask_question(my_question)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise: Test Different Query Types\n",
    "\n",
    "Try these different types of questions:\n",
    "\n",
    "1. **Specific factual questions**: \"What is the maximum financial penalty for PDPA violations?\"\n",
    "2. **Comparative questions**: \"What's the difference between consent and deemed consent?\"\n",
    "3. **Policy questions**: \"What should I do if an employee violates the code of conduct?\"\n",
    "4. **Edge cases**: \"How do I handle customer complaints?\" (tests routing)\n",
    "\n",
    "Observe:\n",
    "- Which domain gets selected\n",
    "- How relevant the retrieved chunks are\n",
    "- Answer quality and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 12: Advanced Features & Improvements\n",
    "\n",
    "Let's explore some advanced techniques to improve the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Feature 1: Metadata Filtering\n",
    "\n",
    "Retrieve documents from specific sources or page ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def rag_with_filter(question: str, domain: str, filename_filter: str = None):\n",
    "    \"\"\"\n",
    "    RAG with metadata filtering.\n",
    "    \n",
    "    Args:\n",
    "        question: Query\n",
    "        domain: Domain to search\n",
    "        filename_filter: Optional filename to filter by\n",
    "    \"\"\"\n",
    "    retriever = vectorstores[domain].as_retriever(\n",
    "        search_kwargs={\n",
    "            \"k\": 3,\n",
    "            \"filter\": {\"filename\": filename_filter} if filename_filter else None\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    docs = retriever.invoke(question)\n",
    "    print(f\"üìö Retrieved {len(docs)} chunks from {domain}\")\n",
    "    for doc in docs:\n",
    "        print(f\"  ‚Ä¢ {doc.metadata['filename']} (Page {doc.metadata.get('page', '?')})\")\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Example: Search only in enforcement guidelines\n",
    "# filtered_docs = rag_with_filter(\n",
    "#     \"What are the penalties?\",\n",
    "#     \"legal\",\n",
    "#     \"Advisory Guidelines on Enforcement of DP Provisions_1oct2022.pdf\"\n",
    "# )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Feature 2: Confidence Scoring\n",
    "\n",
    "Add confidence scores to answers based on retrieval scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def rag_with_confidence(question: str) -> Dict:\n",
    "    \"\"\"\n",
    "    RAG with confidence scoring based on retrieval similarity.\n",
    "    \"\"\"\n",
    "    # Route query\n",
    "    selected_route = router_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Retrieve with scores\n",
    "    vectorstore = vectorstores[selected_route]\n",
    "    docs_with_scores = vectorstore.similarity_search_with_score(question, k=3)\n",
    "    \n",
    "    # Calculate average similarity score (lower is better for cosine distance)\n",
    "    avg_score = sum(score for _, score in docs_with_scores) / len(docs_with_scores)\n",
    "    \n",
    "    # Convert to confidence (inverse of distance, normalized)\n",
    "    confidence = max(0, min(1, 1 - (avg_score / 2)))  # Normalize to 0-1\n",
    "    \n",
    "    print(f\"üéØ Confidence Score: {confidence:.2f}\")\n",
    "    print(f\"üìä Retrieval Scores: {[f'{score:.3f}' for _, score in docs_with_scores]}\")\n",
    "    \n",
    "    return {\n",
    "        \"confidence\": confidence,\n",
    "        \"docs\": [doc for doc, _ in docs_with_scores],\n",
    "        \"scores\": [score for _, score in docs_with_scores]\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = rag_with_confidence(\"What are the data protection principles?\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Feature 3: Fallback Strategy\n",
    "\n",
    "If confidence is low, try searching multiple domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def rag_with_fallback(question: str, confidence_threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    RAG with fallback to other domains if confidence is low.\n",
    "    \"\"\"\n",
    "    # Try primary route\n",
    "    primary_route = router_chain.invoke({\"question\": question})\n",
    "    primary_result = rag_with_confidence(question)\n",
    "    \n",
    "    if primary_result[\"confidence\"] < confidence_threshold:\n",
    "        print(f\"‚ö†Ô∏è Low confidence ({primary_result['confidence']:.2f}), trying other domains...\")\n",
    "        \n",
    "        # Try all other domains\n",
    "        all_docs = []\n",
    "        for domain in vectorstores.keys():\n",
    "            docs = vectorstores[domain].similarity_search(question, k=1)\n",
    "            all_docs.extend(docs)\n",
    "        \n",
    "        print(f\"üìö Expanded search: retrieved {len(all_docs)} chunks from all domains\")\n",
    "        return all_docs\n",
    "    \n",
    "    return primary_result[\"docs\"]\n",
    "\n",
    "# Example:\n",
    "# docs = rag_with_fallback(\"Tell me about workplace safety\", confidence_threshold=0.6)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 13: Evaluation Framework\n",
    "\n",
    "Let's create a simple evaluation framework to assess answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define evaluation criteria\n",
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert evaluator assessing the quality of RAG system answers.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "Retrieved Context: {context}\n",
    "\n",
    "Evaluate the answer on these criteria (score 1-5 for each):\n",
    "\n",
    "1. ACCURACY: Is the answer factually correct based on the context?\n",
    "2. COMPLETENESS: Does the answer fully address the question?\n",
    "3. RELEVANCE: Is the answer relevant to the question?\n",
    "4. CLARITY: Is the answer clear and well-structured?\n",
    "\n",
    "Provide scores in this format:\n",
    "ACCURACY: X/5\n",
    "COMPLETENESS: X/5\n",
    "RELEVANCE: X/5\n",
    "CLARITY: X/5\n",
    "OVERALL: X/5\n",
    "\n",
    "Brief explanation (1-2 sentences):\n",
    "\"\"\")\n",
    "\n",
    "def evaluate_answer(question: str, answer: str, context: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate answer quality using LLM.\n",
    "    \"\"\"\n",
    "    eval_chain = eval_prompt | llm | StrOutputParser()\n",
    "    evaluation = eval_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"context\": context\n",
    "    })\n",
    "    \n",
    "    # Parse scores\n",
    "    import re\n",
    "    scores = {}\n",
    "    for criterion in [\"ACCURACY\", \"COMPLETENESS\", \"RELEVANCE\", \"CLARITY\", \"OVERALL\"]:\n",
    "        match = re.search(f\"{criterion}:\\s*(\\d+)/5\", evaluation)\n",
    "        if match:\n",
    "            scores[criterion.lower()] = int(match.group(1))\n",
    "    \n",
    "    return {\n",
    "        \"scores\": scores,\n",
    "        \"evaluation_text\": evaluation\n",
    "    }\n",
    "\n",
    "# Evaluate all previous results\n",
    "print(\"\\nüìä EVALUATING ANSWERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "evaluation_results = []\n",
    "for result in all_results[:2]:  # Evaluate first 2 to save time\n",
    "    context = format_docs_with_citations([{\"page_content\": s[\"content_preview\"], \"metadata\": s} for s in result[\"sources\"]])\n",
    "    eval_result = evaluate_answer(\n",
    "        result[\"question\"],\n",
    "        result[\"answer\"],\n",
    "        context\n",
    "    )\n",
    "    evaluation_results.append(eval_result)\n",
    "    \n",
    "    print(f\"\\n‚ùì {result['question'][:60]}...\")\n",
    "    print(f\"Scores: {eval_result['scores']}\")\n",
    "\n",
    "# Calculate average scores\n",
    "if evaluation_results:\n",
    "    avg_scores = {}\n",
    "    for criterion in [\"accuracy\", \"completeness\", \"relevance\", \"clarity\", \"overall\"]:\n",
    "        scores = [r[\"scores\"].get(criterion, 0) for r in evaluation_results]\n",
    "        avg_scores[criterion] = sum(scores) / len(scores) if scores else 0\n",
    "    \n",
    "    print(f\"\\nüìà Average Scores:\")\n",
    "    for criterion, score in avg_scores.items():\n",
    "        print(f\"  {criterion.capitalize()}: {score:.2f}/5\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 14: Key Learnings & Best Practices\n",
    "\n",
    "### üéì What You've Learned:\n",
    "\n",
    "1. **Multi-Index Architecture**: Separate indexes improve relevance by domain specialization\n",
    "2. **Intelligent Routing**: LLM-based routing handles complex query classification\n",
    "3. **Document Processing**: Chunking strategies balance context and retrieval precision\n",
    "4. **Citation Tracking**: Metadata enables source attribution and verification\n",
    "5. **Performance Monitoring**: Metrics help identify bottlenecks and optimize\n",
    "\n",
    "### ‚úÖ Best Practices:\n",
    "\n",
    "1. **Domain Design**:\n",
    "   - Create clear, distinct domains\n",
    "   - Ensure routing descriptions are specific\n",
    "   - Include fallback/general category\n",
    "\n",
    "2. **Chunking Strategy**:\n",
    "   - Balance chunk size (800-1200 chars optimal)\n",
    "   - Use overlap to preserve context\n",
    "   - Respect document structure (paragraphs, sections)\n",
    "\n",
    "3. **Retrieval Optimization**:\n",
    "   - Start with k=3 chunks, adjust based on domain\n",
    "   - Use metadata filtering for specific sources\n",
    "   - Implement confidence thresholds\n",
    "\n",
    "4. **Answer Quality**:\n",
    "   - Include source citations in prompts\n",
    "   - Set clear answer guidelines\n",
    "   - Handle \"no answer found\" gracefully\n",
    "\n",
    "5. **Monitoring**:\n",
    "   - Track routing accuracy\n",
    "   - Monitor retrieval relevance\n",
    "   - Measure end-to-end latency\n",
    "   - Evaluate answer quality regularly\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. Add more documents to existing domains\n",
    "2. Create new domain indexes as needed\n",
    "3. Implement user feedback collection\n",
    "4. Fine-tune routing decisions based on metrics\n",
    "5. Explore hybrid search (semantic + keyword)\n",
    "6. Add query rewriting for better retrieval\n",
    "7. Implement caching for common queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 15: Cleanup & Save Results\n",
    "\n",
    "Let's save our results and clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save results to JSON\n",
    "results_file = f\"rag_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "output_data = {\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"num_domains\": len(vectorstores),\n",
    "        \"domains\": list(vectorstores.keys()),\n",
    "        \"total_queries\": len(all_results)\n",
    "    },\n",
    "    \"document_stats\": document_stats,\n",
    "    \"chunk_stats\": chunk_stats,\n",
    "    \"results\": all_results,\n",
    "    \"performance_summary\": {\n",
    "        \"avg_total_time\": metrics_df['Total Time (s)'].mean(),\n",
    "        \"avg_route_time\": metrics_df['Route Time (s)'].mean(),\n",
    "        \"avg_retrieval_time\": metrics_df['Retrieval Time (s)'].mean(),\n",
    "        \"avg_generation_time\": metrics_df['Generation Time (s)'].mean(),\n",
    "    },\n",
    "    \"routing_distribution\": routing_distribution\n",
    "}\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {results_file}\")\n",
    "print(f\"\\nüìä Session Summary:\")\n",
    "print(f\"  ‚Ä¢ Processed {sum(stats['num_files'] for stats in document_stats.values())} PDF files\")\n",
    "print(f\"  ‚Ä¢ Created {len(vectorstores)} domain indexes\")\n",
    "print(f\"  ‚Ä¢ Answered {len(all_results)} queries\")\n",
    "print(f\"  ‚Ä¢ Average response time: {metrics_df['Total Time (s)'].mean():.3f}s\")\n",
    "\n",
    "print(\"\\nüéâ Tutorial Complete! You've mastered Multi-Index RAG with PDF documents!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "### Exercise 1: Add a New Domain\n",
    "1. Add new PDFs to a `pdf_documents/` subfolder (e.g., `technical/`)\n",
    "2. Update the domain mapping\n",
    "3. Create a new vector index\n",
    "4. Update the routing schema\n",
    "5. Test with relevant queries\n",
    "\n",
    "### Exercise 2: Improve Chunking\n",
    "1. Experiment with different chunk sizes (500, 1500, 2000)\n",
    "2. Try different overlap values (0, 100, 300)\n",
    "3. Compare retrieval quality\n",
    "4. Document your findings\n",
    "\n",
    "### Exercise 3: Enhanced Routing\n",
    "1. Modify RouteQuery to include confidence scores\n",
    "2. Implement multi-domain retrieval for ambiguous queries\n",
    "3. Add query intent classification (factual vs. procedural)\n",
    "4. Log routing decisions for analysis\n",
    "\n",
    "### Exercise 4: Build a Q&A Interface\n",
    "1. Create a simple web UI with Streamlit/Gradio\n",
    "2. Display routing decisions visually\n",
    "3. Show source citations with clickable links\n",
    "4. Add query history and favorites\n",
    "\n",
    "### Exercise 5: Production Readiness\n",
    "1. Add error handling and retries\n",
    "2. Implement caching layer\n",
    "3. Add rate limiting\n",
    "4. Create health check endpoints\n",
    "5. Add comprehensive logging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
